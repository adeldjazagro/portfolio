# ADEL - Business Analyst / Product Owner

**5 ans d'expérience** | Transformation Data & Digitale | Secteurs Publics, Énergie, Banque

---

# PROFIL

**Business Analyst / Product Owner** spécialisé dans la **transformation data et l'intégration de systèmes complexes**, avec un track record de **5 ans** dans des environnements exigeants (secteur public, énergie, banque, conseil).

Expert en **cadrage métier, définition de vision produit et pilotage Agile** de solutions à fort impact business. Reconnu pour ma capacité à :
- **Traduire les enjeux stratégiques** en roadmaps produit actionnables
- **Prioriser par la valeur** et maximiser le ROI des initiatives data/IT
- **Piloter des équipes cross-fonctionnelles** vers des livrables mesurables
- **Arbitrer les trade-offs** métier/tech grâce à une forte expertise technique (Data, APIs, Intégration)

Mon différenciateur : une **double compétence fonctionnelle et technique** qui me permet de challenger les architectures, d'accélérer les décisions produit et d'assurer la faisabilité des solutions proposées.

**Résultats clés** : Réduction de 30-40% des délais opérationnels, automatisation de processus critiques, livraison de 15+ features majeures à fort impact métier.

---

# VALEUR & IMPACT DÉLIVRÉ

| Domaine | Réalisations mesurables |
|---------|------------------------|
| **Gains opérationnels** | Réduction de **30% des délais de traitement** (DGEF) et **40% des latences** (ENEDIS) |
| **Automatisation** | Déploiement de **5 intégrations bancaires critiques** éliminant des processus manuels (RCI Bank) |
| **Fiabilité** | **0 incident majeur** sur 18 mois de production sur plateformes critiques (ENEDIS) |
| **Time-to-market** | Livraison incrémentale accélérant la mise en production de **25%** (DGEF) |
| **ROI Data** | Conception de cas d'usage Snowflake générant des insights exploitables sous **48h** vs 2 semaines (Accenture) |

---

# COMPÉTENCES CLÉS

## Product Ownership & Business Analysis

**Stratégie Produit**
- Définition de vision produit et roadmaps orientées valeur métier
- Analyse de marché, benchmark et identification d'opportunités
- Priorisation par la valeur (WSJF, MoSCoW, ROI)
- Construction et arbitrage de business cases

**Gestion de Backlog & Livraison**
- Rédaction de user stories, epics, features avec critères d'acceptation INVEST
- Gestion et priorisation de backlog produit (valeur / effort / risque)
- Définition et suivi de KPIs produit (adoption, performance, satisfaction)
- Pilotage de la livraison incrémentale et des releases

**Collaboration & Facilitation**
- Animation d'ateliers métiers (workshops, Design Thinking, Event Storming)
- Recueil et analyse des besoins auprès de stakeholders multiples
- Interface entre équipes métiers (finance, ops, RH) et techniques
- Gestion des attentes et communication produit

## Méthodologies Agiles & Frameworks

- **Agile / Scrum** : Cérémonies, coaching équipes, amélioration continue
- **SAFe** : Program Increment Planning, ART sync, gestion des dépendances
- **Outils** : JIRA (configuration, tableaux de bord), Confluence, Miro, Figma
- **Delivery** : CI/CD, définition de Definition of Done/Ready

## Domaines Fonctionnels d'Expertise

**Data & Analytics**
- Cas d'usage data : BI, analytics, reporting, data quality
- Gouvernance des données et conformité (RGPD, SOX)
- Modélisation conceptuelle et logique de données
- Architecture data (lacs de données, entrepôts, streaming)

**Intégration & APIs**
- Stratégies d'intégration (temps réel, batch, événementiel)
- APIs REST/SOAP : conception, spécifications OpenAPI
- Patterns d'intégration (EIP, messaging, event-driven)
- Middleware et ESB

## Compétences Techniques (différenciateur)

**Écosystèmes Data**
- **Snowflake** : Modélisation, performance, gouvernance, Cortex (LLM, Search)
- **Big Data** : Spark, PySpark, architectures distribuées
- **Streaming** : Kafka, Kafka Streams, KSQL, événementiel

**Langages & Outils**
- **SQL avancé** (requêtes complexes, optimisation)
- **Python** (data manipulation, scripting)
- Notions : Java, Apache Camel, Spring Boot

**Cloud & DevOps**
- GCP (BigQuery), Azure (en cours de certification)
- Docker, Git/GitLab, CI/CD
- Observabilité : Datadog, monitoring

> *Ces compétences techniques me permettent de comprendre les contraintes d'implémentation, de challenger les estimations, et d'arbitrer efficacement entre solutions concurrentes.*

---

# EXPÉRIENCES PROFESSIONNELLES

## Accenture France – **Product Owner Data & Analytics**
**Juin 2024 – Aujourd'hui** | Paris

### Contexte & Enjeux
Pilotage produit de cas d'usage data stratégiques sur Snowflake pour des clients grands comptes (finance, opérations, pilotage). Enjeux : accélérer le time-to-insight, réduire les coûts d'infrastructure, et démocratiser l'accès aux données.

### Réalisations Product Owner
- **Vision & Cadrage** : Co-construction de la vision produit avec les sponsors métiers (finance, ops) et définition de roadmaps data sur 6-12 mois
- **Cas d'usage IA** : Cadrage et livraison de 3 cas d'usage **Snowflake Cortex** (LLM + Search) automatisant l'analyse de données et réduisant le time-to-insight de **2 semaines à 48h**
- **Backlog & Priorisation** : Gestion de backlog multi-sources (10+ stakeholders) avec priorisation par la valeur métier et faisabilité technique
- **ROI & Performance** : Optimisation des coûts Snowflake (warehouses, storage) suite à l'analyse des patterns d'usage, générant **20% d'économies** trimestrielles
- **Delivery Agile** : Animation de sprints (planning, reviews, retros) avec équipes data (data engineers, analysts, ML engineers)

### Contribution Technique
- Accompagnement des équipes sur la modélisation data (star schema, data vault) et l'optimisation SQL
- Validation de POCs et architecture des pipelines de données (ingestion, transformation, orchestration)
- Contribution au développement de pipelines critiques (Python, SQL, Spark)

**Impact** : 3 cas d'usage data en production, adoption par 50+ utilisateurs métiers, satisfaction produit de 4.5/5

**Environnement** : Snowflake, Cortex, SQL, Python, Spark, Airflow, JIRA, Agile

---

## Ministère de l'Intérieur / DGEF – **Business Analyst & Product Owner Data**
**Juin 2024 – Aujourd'hui** | Paris

### Contexte & Enjeux
Transformation digitale d'une direction opérationnelle avec des besoins data complexes (ingestion multi-sources, conformité RGPD, contraintes réglementaires). Objectif : fiabiliser et accélérer les processus de traitement de données critiques.

### Réalisations BA/PO
- **Recueil & Analyse** : Conduite d'ateliers métiers avec 5 directions pour cartographier les besoins data et identifier les quick wins
- **Formalisation** : Rédaction de 30+ user stories détaillées avec critères d'acceptation, mockups et règles de gestion complexes
- **Roadmap Produit** : Construction d'une roadmap sur 12 mois alignée sur les priorités opérationnelles et les contraintes budgétaires
- **Pilotage Agile** : Gestion du backlog et animation de sprints avec une équipe de 6 personnes (dev, data engineers, ops)
- **Livraison Incrémentale** : Mise en production de 4 pipelines data majeurs en mode itératif, avec feedback continu des utilisateurs
- **Impact Opérationnel** : **Réduction de 30% des délais de traitement** et élimination de processus manuels sujets à erreurs

### Contribution Technique
- Participation à la conception des architectures de pipelines (PySpark, APIs REST/SOAP, MongoDB)
- Support à la résolution de blocages techniques (performance, intégration, qualité des données)

**Impact** : 4 produits data en production, 100+ utilisateurs métiers impactés, conformité RGPD assurée

**Environnement** : PySpark, Python, SQL, REST/SOAP, JIRA, Confluence, Agile

---

## RCI Bank / Mobilize – **Business Analyst Intégrations Bancaires**
**Janvier 2024 – Juin 2024** | Paris

### Contexte & Enjeux
Modernisation des intégrations bancaires critiques (paiements, gestion de risque, conformité) dans un contexte réglementaire contraint. Nécessité de fiabiliser les flux existants et d'en automatiser de nouveaux.

### Réalisations Business Analyst
- **Interface Métier/IT** : Point de contact principal entre les équipes métiers (finance, risque, compliance) et les équipes techniques d'intégration
- **Expression de Besoins** : Rédaction de 25+ user stories détaillées pour des flux d'intégration complexes (APIs bancaires, ERP, CRM)
- **Spécifications Fonctionnelles** : Documentation des règles métiers, mappings de données, et scenarios de test fonctionnels
- **Coordination & Suivi** : Pilotage des livrables avec les équipes offshore, suivi des jalons et reporting de l'avancement aux sponsors
- **Mises en Production** : Accompagnement de **5 intégrations critiques** en production avec **0 rollback**, automatisant des processus manuels clés

### Contribution Technique
- Compréhension approfondie des architectures d'intégration (Apache Camel, APIs REST)
- Support au dépannage de flux (analyse de logs, debug)

**Impact** : 5 intégrations livrées, automatisation de processus financiers à fort enjeu, conformité réglementaire respectée

**Environnement** : Java, Apache Camel, JIRA, Bitbucket, APIs REST, Agile

---

## ENEDIS – **Product Owner Plateforme d'Intégration Temps Réel**
**Juin 2022 – Décembre 2023** | Paris La Défense

### Contexte & Enjeux
Pilotage produit d'une **plateforme d'intégration critique** pour la distribution d'énergie (Kafka, APIs REST), avec des contraintes métiers contradictoires : temps réel vs fiabilité vs coûts. Enjeu : moderniser l'architecture legacy tout en garantissant la continuité de service.

### Réalisations Product Owner
- **Vision & Stratégie** : Définition de la vision produit et de la roadmap sur 18 mois pour la modernisation de 15+ intégrations (migration Kafka, API-first)
- **Arbitrage Métier/Tech** : Conception d'une **architecture hybride temps réel/batch** réduisant les latences de **40%** tout en garantissant la conformité aux exigences de disponibilité (99.9%)
- **Pilotage SAFe** : Animation de 2 équipes cross-fonctionnelles (12 personnes : dev, ops, archi, métiers) dans un cadre SAFe (PI Planning, ART sync)
- **Gestion de Backlog** : Priorisation continue de 100+ user stories selon la valeur métier, les dépendances techniques et les contraintes réglementaires
- **Delivery Excellence** : Livraison de **8 features majeures** permettant l'automatisation de processus métiers critiques, avec **0 incident majeur** sur 18 mois

### Contribution Technique
- Accompagnement des choix d'architecture (Kafka Streams vs KSQL, patterns EIP)
- Validation technique des user stories et critères d'acceptation (faisabilité, estimation)
- Participation aux code reviews et POCs pour des features complexes

**Impact** : Réduction de 40% des latences, +99.9% de disponibilité maintenue, satisfaction des équipes métiers élevée

**Environnement** : SAFe, JIRA, Kafka, Spring Boot, Apache Camel, Docker, Jenkins

---

## Palantir France – **Business Analyst Data**
**Janvier 2022 – Juin 2022** | Paris

### Contexte & Enjeux
Projet de valorisation des données opérationnelles de l'aéroport de San Francisco (SFO) sur Palantir Foundry. Objectif : créer des dashboards décisionnels pour optimiser les opérations aéroportuaires.

### Réalisations BA
- **Cadrage Métier** : Analyse des besoins des équipes opérationnelles (planification, sécurité, logistique)
- **Modélisation Data** : Conception du modèle de données logique et des transformations nécessaires
- **Delivery** : Livraison de dashboards permettant une visibilité temps réel sur les KPIs opérationnels
- **Formation Utilisateurs** : Accompagnement des équipes métiers à l'utilisation de Foundry

**Environnement** : Palantir Foundry, PySpark, SQL, DataViz

---

## CEA – **Data Analyst & Business Intelligence**
**Octobre 2020 – Décembre 2021** | Saclay

### Contexte & Enjeux
Projets R&D combinant blockchain et machine learning pour des cas d'usage industriels. Contribution à la phase d'étude et de prototypage.

### Réalisations
- **Analyse de Faisabilité** : Étude de cas d'usage blockchain pour la traçabilité industrielle
- **Data Science** : Développement de modèles de machine learning (détection d'anomalies, classification)
- **Visualisation** : Création de dashboards pour présenter les résultats aux sponsors

**Environnement** : Python, R, Java, scikit-learn, GitLab

---

## Freelance – **Data Analyst GCP**
**Décembre 2020 – Avril 2021** | Remote

### Réalisations
- **Ingestion & Transformation** : Mise en place de pipelines data sur Google Cloud Platform (BigQuery, Cloud Storage)
- **Machine Learning** : Modèles prédictifs sur BigQuery ML pour des cas d'usage business
- **Visualisation** : Dashboards interactifs pour la prise de décision

**Environnement** : Python, GCP, BigQuery, scikit-learn, Pandas, DataViz

---

# FORMATIONS & CERTIFICATIONS

- **Master Big Data & Fouille de Données** – Université Paris 8 (Diplôme Bac+5)
- **Certification SAFe** – Scaled Agile Framework (ENEDIS)
- **Azure Fundamentals (AZ-900)** – En cours de certification
- **Azure Data Fundamentals (DP-900)** – En cours de certification

**Formations continues** : Design Thinking, Event Storming, API Design, Product Management

---

# LANGUES

- **Français** : Langue maternelle
- **Anglais** : Courant (professionnel et technique)

---

# CENTRES D'INTÉRÊT

- **Veille techno & produit** : Participation à des meetups Product Management et Data (Product Tank Paris, Data For Good)
- **Open Source** : Contribution à des projets data (dbt, Apache Superset)
- **Mentorat** : Accompagnement de junior BA/PO dans leur montée en compétences

---

# DISPONIBILITÉ & MOBILITÉ

- **Disponibilité** : Immédiate
- **Mobilité** : Île-de-France | Remote | Déplacements France possibles
- **Statut** : Freelance / CDI selon opportunités

---

# CONTACT

**LinkedIn** : [Votre profil LinkedIn]
**Email** : [Votre email]
**Téléphone** : [Votre numéro]
**Portfolio** : [Lien vers vos projets si applicable]
